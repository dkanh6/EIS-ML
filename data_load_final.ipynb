{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM2ljGyLyQ5LTxfD2eIeW0o",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dkanh6/EIS-ML/blob/main/data_load_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem at Hand:**\n",
        "We need to extract, clean, and merge experimental data from multiple sources (TXT files and excel files)\n",
        "\n",
        "# **General Data Structure:**\n",
        "1. **TXT Files:**\n",
        "\n",
        "  *  **Header:** Contains metadata\n",
        "\n",
        "  *  **Trace A:** Holds the impedance frequency values and the REAL part of the impedance.\n",
        "\n",
        "  *  **Trace B:** Holds the IMAGINARY part of the impedance *(I hope)*\n",
        "\n",
        "2. **Excel Files:**\n",
        "  Contain full experiment details (contains circuit paramters such as capacitance values in the CPEb-T(+) column) and reference the TXT files via a file path embedded in a “Model:” column.\n",
        "3. **Directory Structure:**\n",
        "  The data is organized in a hierarchical structure under the root folder (Data).\n",
        "\n",
        "  * **Experiments:**\n",
        "    1. Control test (no bacteria)\n",
        "    2. Test 5E2 (questionable death)\n",
        "    3. Test 5E3 (marginal death data)\n",
        "    4. Test 5E4 (death data)\n",
        "    5. Test 5E5 (death data)\n",
        "\n",
        "  * Each experiment represents a different starting bacterial concentration (from 5E2 CFU to 5E5 CFU).\n",
        "\n",
        "  * Within Each Experiment:\n",
        "    * There are 3 Cartridges (Cartridge 1, 2, and 3).\n",
        "    * Each cartridge is measured at 12 timepoints (30 minutes apart).\n",
        "    * For each timepoint, 5 samples are taken (e.g., files named 1-1-1.txt, 1-1-2.txt, … for Cartridge 1 at timepoint 1).\n",
        "\n"
      ],
      "metadata": {
        "id": "_FGTORqPVxdt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LXRwgQW0CcCj",
        "outputId": "fa2b031f-d886-4c53-8d42-ceae01b03a82"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'EIS-ML'...\n",
            "remote: Enumerating objects: 1213, done.\u001b[K\n",
            "remote: Counting objects: 100% (25/25), done.\u001b[K\n",
            "remote: Compressing objects: 100% (15/15), done.\u001b[K\n",
            "remote: Total 1213 (delta 18), reused 10 (delta 10), pack-reused 1188 (from 1)\u001b[K\n",
            "Receiving objects: 100% (1213/1213), 3.32 MiB | 19.53 MiB/s, done.\n",
            "Resolving deltas: 100% (1051/1051), done.\n",
            "/content/EIS-ML/EIS-ML/EIS-ML/EIS-ML/EIS-ML/EIS-ML/EIS-ML\n",
            "No outliers found in this folder.\n",
            "\n",
            "Summary of TXT vs. Excel matches:\n",
            "                      experiment            cartridge  total_txt_files  matched_excel  unmatched  excel_txt_count\n",
            "test-5e5-07242020-G (death data) test-5e5-c1-07242020               51             51          0                0\n",
            "test-5e5-07242020-G (death data) test-5e5-c2-07242020               49             49          0                0\n",
            "test-5e5-07242020-G (death data) test-5e5-c3-07242020               50             50          0                0\n",
            "No outliers found in this folder.\n",
            "\n",
            "Summary of TXT vs. Excel matches:\n",
            "                      experiment            cartridge  total_txt_files  matched_excel  unmatched  excel_txt_count\n",
            "test-5e4-07232020-G (Death data) test-5e4-c1-07232020               50             50          0                0\n",
            "test-5e4-07232020-G (Death data) test-5e4-c2-07232020               50             47          3                0\n",
            "test-5e4-07232020-G (Death data) test-5e4-c3-07232020               50             50          0                0\n",
            "No outliers found in this folder.\n",
            "\n",
            "Summary of TXT vs. Excel matches:\n",
            "                               experiment            cartridge  total_txt_files  matched_excel  unmatched  excel_txt_count\n",
            "test-5e3-07212020-G (marginal death data) test-5e3-c1-07212020               50             49          1                0\n",
            "test-5e3-07212020-G (marginal death data) test-5e3-c2-07212020               55             55          0                0\n",
            "test-5e3-07212020-G (marginal death data) test-5e3-c3-07212020               55             55          0                0\n",
            "No outliers found in this folder.\n",
            "\n",
            "Summary of TXT vs. Excel matches:\n",
            "              experiment           cartridge  total_txt_files  matched_excel  unmatched  excel_txt_count\n",
            "contorl test-no bacteria control-c1-06262020               60             60          0                0\n",
            "contorl test-no bacteria control-c2-06262020               60             60          0                0\n",
            "contorl test-no bacteria control-c3-06262020               59             59          0                0\n",
            "No outliers found in this folder.\n",
            "\n",
            "Summary of TXT vs. Excel matches:\n",
            "   experiment                 cartridge  total_txt_files  matched_excel  unmatched  excel_txt_count\n",
            "control-SA+PA control-SA+PA-C1-06282020               60             60          0                0\n",
            "control-SA+PA control-SA+PA-C2-06282020               60             60          0                0\n",
            "control-SA+PA control-SA+PA-C3-06282020               60             60          0                0\n",
            "No outliers found in this folder.\n",
            "\n",
            "Summary of TXT vs. Excel matches:\n",
            "                              experiment            cartridge  total_txt_files  matched_excel  unmatched  excel_txt_count\n",
            "test-5e2-08052020-G (questionable death) test-5e2-c1-08052020               55             55          0                0\n",
            "test-5e2-08052020-G (questionable death) test-5e2-c2-08052020               55             55          0                0\n",
            "test-5e2-08052020-G (questionable death) test-5e2-c3-08052020               55             55          0                0\n"
          ]
        }
      ],
      "source": [
        "# 1) Clone the repository\n",
        "!git clone https://github.com/dkanh6/EIS-ML.git\n",
        "%cd EIS-ML\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "\n",
        "\n",
        "# for dirpath, dirnames, filenames in os.walk('Data'):\n",
        "#     print('Current Path:', dirpath)\n",
        "#     print('Directories:', dirnames)\n",
        "#     print('Files:', filenames)\n",
        "#     print()\n",
        "\n",
        "\n",
        "def extract_exp_key(filename):\n",
        "    match = re.search(r'(\\d+-\\d+-\\d+)', filename)\n",
        "    if match:\n",
        "        return match.group(1).strip().lower()\n",
        "    return \"\"\n",
        "\n",
        "\n",
        "\n",
        "def parse_path_metadata(filepath):\n",
        "    # Split the complete file path into a list using the OS separator.\n",
        "    parts = filepath.split(os.sep)\n",
        "    # If there are fewer than 3 parts, we cannot extract everything so jsut return defaults.\n",
        "    if len(parts) < 3:\n",
        "        return None, None, None, None, \"\"\n",
        "    # The experiment folder is the third-to-last part in the path.\n",
        "    experiment_folder = parts[-3]\n",
        "    # The cartridge folder is the second-to-last part in the path.\n",
        "    cartridge_folder  = parts[-2]\n",
        "    # Get just the filename (without directory paths).\n",
        "    filename = os.path.basename(filepath)\n",
        "    # Convert the filename to lowercase and extract the experiment key (a pattern like \"2-10-3\") using extract_exp_key.\n",
        "    key = extract_exp_key(filename.lower())\n",
        "    # Remove the \".txt\" extension from the filename, convert to lowercase, and split it by '-'\n",
        "    tokens = filename.lower().replace(\".txt\", \"\").split('-')\n",
        "    # Initialize timepoint and subsample as None.\n",
        "    timepoint = None\n",
        "    subsample = None\n",
        "    # If the filename splits into exactly three tokens, try converting the second and third tokens to integers.\n",
        "    if len(tokens) == 3:\n",
        "        try:\n",
        "            timepoint = int(tokens[1])\n",
        "            subsample = int(tokens[2])\n",
        "        except ValueError:\n",
        "            # If conversion fails, leave timepoint and subsample as None.\n",
        "            pass\n",
        "    # Return a tuple containing the experiment folder, cartridge folder, timepoint, subsample, and experiment key.\n",
        "    return experiment_folder, cartridge_folder, timepoint, subsample, key\n",
        "\n",
        "# Reads a TXT file and extracts impedance data from two traces A and B\n",
        "def parse_txt_file(filepath):\n",
        "    # Open the file in read mode and read all its lines into a list\n",
        "    with open(filepath, 'r') as f:\n",
        "        lines = f.readlines()\n",
        "\n",
        "    # Create two dictionaries to hold impedance data for two traces:\n",
        "    # Trace A (usually contains the \"real\" part)\n",
        "    # Trace B (due to formatting issues, it represents the \"imaginary\" component) ( god I hope)\n",
        "    trace_A = {'frequency': [], 'real': [], 'imag': []}\n",
        "    trace_B = {'frequency': [], 'real': [], 'imag': []}\n",
        "\n",
        "    # Initialize a variable to keep track of which trace (A or B) we are currently reading.\n",
        "    current_trace = None\n",
        "\n",
        "    # A flag to indicate whether we are in the data section (after the header \"Frequency\")\n",
        "    reading_data = False\n",
        "\n",
        "    # Loop through every line in the file.\n",
        "    for line in lines:\n",
        "        # Remove any extra whitespace and newline characters from the line.\n",
        "        line = line.strip()\n",
        "\n",
        "        # Check if the line indicates the start of a new trace.\n",
        "        if line.startswith('\"TRACE:'):\n",
        "            # If the line contains \"A\", set the current trace to 'A'\n",
        "            if \"A\" in line:\n",
        "                current_trace = 'A'\n",
        "            # If the line contains \"B\", set the current trace to 'B'\n",
        "            elif \"B\" in line:\n",
        "                current_trace = 'B'\n",
        "            # Reset the data reading flag since we're starting a new trace.\n",
        "            reading_data = False\n",
        "            # Skip the rest of this loop iteration.\n",
        "            continue\n",
        "\n",
        "        # If the line starts with '\"FORMAT:\"', we ignore it (not part of the data)\n",
        "        if line.startswith('\"FORMAT:'):\n",
        "            reading_data = False\n",
        "            continue\n",
        "\n",
        "        # If the line starts with '\"Frequency\"', it signals that the actual data follows.\n",
        "        if line.startswith('\"Frequency\"'):\n",
        "            reading_data = True\n",
        "            continue\n",
        "\n",
        "        # Skip any empty lines.\n",
        "        if not line:\n",
        "            continue\n",
        "\n",
        "        # If we're in the data section and have determined the current trace:\n",
        "        if reading_data and current_trace is not None:\n",
        "            # Split the line by whitespace into parts. We expect at least 3 parts: frequency, real, imag.\n",
        "            parts = line.split()\n",
        "            if len(parts) >= 3:\n",
        "                try:\n",
        "                    # Convert the first part to a float (frequency)\n",
        "                    freq = float(parts[0])\n",
        "                    # Convert the second part to a float (real value)\n",
        "                    real_val = float(parts[1])\n",
        "                    # Convert the third part to a float (imaginary value)\n",
        "                    imag_val = float(parts[2])\n",
        "\n",
        "                    # Depending on the current trace, append these values to the corresponding dictionary.\n",
        "                    if current_trace == 'A':\n",
        "                        trace_A['frequency'].append(freq)\n",
        "                        trace_A['real'].append(real_val)\n",
        "                        trace_A['imag'].append(imag_val)\n",
        "                    else:\n",
        "                        trace_B['frequency'].append(freq)\n",
        "                        trace_B['real'].append(real_val)\n",
        "                        trace_B['imag'].append(imag_val)\n",
        "                except ValueError:\n",
        "                    # If any conversion fails (i.e., the line does not contain proper numbers), skip the line.\n",
        "                    continue\n",
        "\n",
        "    # Return the two dictionaries containing the parsed impedance data.\n",
        "    return trace_A, trace_B\n",
        "\n",
        "\n",
        "\n",
        "# Function: build_master_dataframe\n",
        "# Walks through all subdirectories of base_dir (using os.walk) and processes each TXT file (ignoring files containing \"placeholder\" vestigial tail I havent removed in some foldes when uplaoding to github).\n",
        "# For each text file it extracts metadata and impedance data, then returns a DataFrame of all records.\n",
        "# -------------------------------------------------------------------\n",
        "def build_master_dataframe(base_dir):\n",
        "    samples = [] # Initialize an empty list to hold a record (dictionary) for each TXT file.\n",
        "\n",
        "    # Walk through all directories and subdirectories starting at base_dir.\n",
        "    for root, dirs, files in os.walk(base_dir):\n",
        "\n",
        "        # Iterate over each file in the current directory.\n",
        "        for file in files:\n",
        "            # Check if the file name (in lowercase) ends with '.txt' and does not contain \"placeholder\".\n",
        "            if file.lower().endswith(\".txt\") and \"placeholder\" not in file.lower():\n",
        "\n",
        "                # Construct the full path to the file by joining the current root directory with the file name.\n",
        "                fp = os.path.join(root, file)\n",
        "\n",
        "                # Extract metadata from the file path:\n",
        "                # This includes: experiment (folder 3 from the end), cartridge (folder 2 from the end),\n",
        "                # timepoint and subsample (parsed from the filename), and an experiment key (exp_key).\n",
        "                experiment, cartridge, timepoint, subsample, exp_key = parse_path_metadata(fp)\n",
        "\n",
        "                # If essential metadata (experiment or cartridge) is missing, skip processing this file.\n",
        "                if experiment is None or cartridge is None:\n",
        "                    continue\n",
        "\n",
        "                # Parse the TXT file to extract impedance data.\n",
        "                # The file is expected to contain two traces:Trace A & B (real and img (hopefully))\n",
        "                trace_A, trace_B = parse_txt_file(fp)\n",
        "\n",
        "                # Create a dictionary that stores all the extracted information.\n",
        "                record = {\n",
        "                    \"experiment\": experiment,    # The experiment folder name.\n",
        "                    \"cartridge\": cartridge,      # The cartridge folder name.\n",
        "                    \"timepoint\": timepoint,      # The timepoint parsed from the filename.\n",
        "                    \"subsample\": subsample,      # The subsample number parsed from the filename.\n",
        "                    \"trace_A\": trace_A,          # Dictionary with frequency, real, and imag for Trace A.\n",
        "                    \"trace_B\": trace_B,          # Dictionary with frequency, real, and imag for Trace B.\n",
        "                    \"filepath\": fp,              # Full path to the TXT file.\n",
        "                    \"exp_key\": exp_key           # Extracted experiment key from the filename.\n",
        "                }\n",
        "\n",
        "                # Append the record to the list of samples.\n",
        "                samples.append(record)\n",
        "\n",
        "    # Convert the list of records (dictionaries) into a Pandas DataFrame.\n",
        "    return pd.DataFrame(samples)\n",
        "\n",
        "\n",
        "# Reads an Excel file It processes# sheets 2-4 (ignoring the first sheet to get to the cartridge sheets)\n",
        "def parse_excel_full_data(excel_path):\n",
        "    # Open the Excel file and get information about its sheets.\n",
        "    xls = pd.ExcelFile(excel_path)\n",
        "    # If the file doesn't have at least 4 sheets, return an empty DataFrame.\n",
        "    if len(xls.sheet_names) < 4:\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    # We are going to process sheets 2, 3, and 4 (ignoring the first sheet).\n",
        "    data_sheets = xls.sheet_names[1:4]\n",
        "    # Create an empty list to collect DataFrames from each processed sheet.\n",
        "    all_rows = []\n",
        "\n",
        "    # Loop over each sheet we want to process.\n",
        "    for sheet in data_sheets:\n",
        "        # Read the sheet into a DataFrame, skipping the first 16 rows (headers/extra info).\n",
        "        df_sheet = pd.read_excel(xls, sheet_name=sheet, skiprows=16, header=0)\n",
        "        # If the DataFrame is empty, skip this sheet.\n",
        "        if df_sheet.empty:\n",
        "            continue\n",
        "        # Find any column name that starts with \"model:\" (case-insensitive).\n",
        "        model_cols = [c for c in df_sheet.columns if c.lower().startswith(\"model:\")]\n",
        "        # If such a column exists, rename the first one to \"Model\"; otherwise, skip this sheet.\n",
        "        if model_cols:\n",
        "            df_sheet.rename(columns={model_cols[0]: \"Model\"}, inplace=True)\n",
        "        else:\n",
        "            continue\n",
        "\n",
        "        # Convert the \"Model\" column values to strings.\n",
        "        df_sheet[\"Model\"] = df_sheet[\"Model\"].astype(str)\n",
        "        # Create a mask for rows where \"Model\" contains \".txt\" (ignoring case).\n",
        "        mask = df_sheet[\"Model\"].str.lower().str.contains(\".txt\", na=False)\n",
        "        # Use the mask to filter the DataFrame.\n",
        "        sub = df_sheet[mask].copy()\n",
        "        # If no rows match, skip this sheet.\n",
        "        if sub.empty:\n",
        "            continue\n",
        "        # Extract a file path from the \"Model\" column using a regex pattern.\n",
        "        sub[\"filepath\"] = sub[\"Model\"].str.lower().str.extract(r'(d:.*\\.txt)', expand=False)\n",
        "        # Add a new column with the sheet name.\n",
        "        sub[\"sheet_name\"] = sheet\n",
        "        # Add a new column containing the Excel file path.\n",
        "        sub[\"excel_file\"] = excel_path\n",
        "        # Extract the experiment key from the basename of the file path using extract_exp_key.\n",
        "        sub[\"exp_key\"] = sub[\"filepath\"].apply(lambda x: extract_exp_key(os.path.basename(x)) if pd.notnull(x) else \"\")\n",
        "        # For each column that might contain vestigial \"average\" rows, remove rows where the value is \"average\".\n",
        "        for col_to_check in [\"Unnamed: 24\", \"Concentration (CFU/ml)\"]:\n",
        "            if col_to_check in sub.columns:\n",
        "                sub = sub[~sub[col_to_check].astype(str).str.lower().eq(\"average\")]\n",
        "        # Drop duplicate rows based on the \"filepath\" column.\n",
        "        sub = sub.drop_duplicates(subset=[\"filepath\"])\n",
        "        # Add the processed DataFrame for this sheet to our list.\n",
        "        all_rows.append(sub)\n",
        "\n",
        "    # If we processed any sheets, concatenate all the DataFrames into one; otherwise, return an empty DataFrame.\n",
        "    if all_rows:\n",
        "        return pd.concat(all_rows, ignore_index=True)\n",
        "    else:\n",
        "        return pd.DataFrame()\n",
        "\n",
        "\n",
        "# Merges the TXT and Excel DataFrames based on \"exp_key\" and updates a global\n",
        "def merge_txt_excel_data(df_txt, df_excel):\n",
        "    # Merge the DataFrame from TXT files (df_txt) with the one from Excel (df_excel)\n",
        "    # on the common column \"exp_key\" using a left join (all TXT rows kept).\n",
        "    merged_df = pd.merge(df_txt, df_excel, on=\"exp_key\", how=\"left\", suffixes=(\"\", \"_excel\"))\n",
        "    # Update (or create) a global DataFrame named df_global_merged for later analysis.\n",
        "    global df_global_merged\n",
        "    try:\n",
        "        # If df_global_merged already exists, concatenate the new merged DataFrame to it.\n",
        "        df_global_merged = pd.concat([df_global_merged, merged_df], ignore_index=True)\n",
        "    except NameError:\n",
        "        # Otherwise, create it as a copy of the current merged DataFrame.\n",
        "        df_global_merged = merged_df.copy()\n",
        "    # Return the merged DataFrame.\n",
        "    return merged_df\n",
        "\n",
        "\n",
        "# Applies the 3-sigma rule to detect outliers.\n",
        "def is_outlier_3sigma(values, z_threshold=2):\n",
        "    # Convert the input values to numeric (non-convertible values become NaN).\n",
        "    vals = pd.to_numeric(values, errors='coerce')\n",
        "    # Calculate the mean of these numeric values.\n",
        "    mean_ = vals.mean()\n",
        "    # Calculate the sample standard deviation (with ddof=1) of the values.\n",
        "    std_ = vals.std(ddof=1)\n",
        "    # If the standard deviation is 0 or not a number, return a Series of False (no outliers).\n",
        "    if std_ == 0 or np.isnan(std_):\n",
        "        return pd.Series([False] * len(vals), index=vals.index)\n",
        "    # Compute z-scores for each value: (value - mean) divided by the standard deviation.\n",
        "    z_scores = (vals - mean_) / std_\n",
        "    # Return a boolean Series: True where the absolute z-score is greater than the threshold.\n",
        "    return abs(z_scores) > z_threshold\n",
        "\n",
        "\n",
        "# Processes one experiment folder by:\n",
        "#   1) Building the TXT DataFrame.\n",
        "#   2) Finding and parsing one Excel file.\n",
        "#   3) Merging the TXT and Excel data.\n",
        "#   4) Dropping unwanted columns.\n",
        "#   5) Checking for missing capacitance data and marking valid rows.\n",
        "#   6) Detecting outliers in the \"CPEb-T(+)\" column.\n",
        "#   7) Printing ONLY the outlier summary and the final TXT vs. Excel match summary for ease in quickly determining if things are all okay\n",
        "def process_experiment_folder(experiment_dir):\n",
        "    # Step 1: Build the TXT DataFrame by processing all TXT files in the given experiment folder.\n",
        "    # build_master_dataframe uses os.walk to scan all subdirectories and extract data from each TXT file.\n",
        "    df_txt = build_master_dataframe(experiment_dir)\n",
        "\n",
        "    # If no TXT files were found, exit the function.\n",
        "    if df_txt.empty:\n",
        "        return\n",
        "\n",
        "    # Step 2: Find exactly one Excel (.xlsx) file in the folder.\n",
        "    # Use glob to look for any Excel file in the experiment directory.\n",
        "    excel_files = glob.glob(os.path.join(experiment_dir, \"*.xlsx\"))\n",
        "    # If no Excel file is found, exit the function.\n",
        "    if not excel_files:\n",
        "        return\n",
        "    # Select the first Excel file found.\n",
        "    excel_file_path = excel_files[0]\n",
        "\n",
        "    # Step 3: Parse the Excel file to extract its data into a DataFrame.\n",
        "    # parse_excel_full_data reads sheets 2-4, extracts relevant rows, and creates a DataFrame.\n",
        "    df_excel = parse_excel_full_data(excel_file_path)\n",
        "\n",
        "    # If the Excel DataFrame is not empty, further extract additional metadata (experiment and cartridge)\n",
        "    # from the \"filepath\" column using parse_path_metadata.\n",
        "    if not df_excel.empty:\n",
        "        parse_cols = df_excel[\"filepath\"].apply(parse_path_metadata)\n",
        "        # The first element of the tuple is the experiment folder.\n",
        "        df_excel[\"excel_experiment\"] = parse_cols.apply(lambda x: x[0])\n",
        "        # The second element of the tuple is the cartridge folder.\n",
        "        df_excel[\"excel_cartridge\"]  = parse_cols.apply(lambda x: x[1])\n",
        "    else:\n",
        "        # If the Excel DataFrame is empty, set the new columns to NaN.\n",
        "        df_excel[\"excel_experiment\"] = np.nan\n",
        "        df_excel[\"excel_cartridge\"]  = np.nan\n",
        "\n",
        "    # Step 4: Merge the TXT and Excel DataFrames using the common key \"exp_key\".\n",
        "    # merge_txt_excel_data performs a left join (keeping all TXT records) and also updates a global merged DataFrame.\n",
        "    merged = merge_txt_excel_data(df_txt, df_excel)\n",
        "\n",
        "    # Remove duplicate TXT file records based on the \"filepath\" column.\n",
        "    merged_unique = merged.drop_duplicates(subset=[\"filepath\"]).copy()\n",
        "\n",
        "    # Drop columns that are not needed for further analysis.\n",
        "    unwanted_cols = [\"excel_experiment\", \"excel_cartridge\", \"Unnamed: 24\", \"Concentration (CFU/ml)\"]\n",
        "    merged_unique.drop(columns=unwanted_cols, inplace=True, errors=\"ignore\")\n",
        "\n",
        "    # Step 5: Process the capacitance data.\n",
        "    # Convert the \"CPEb-T(+)\" column values to numeric (non-numeric values will become NaN).\n",
        "    merged_unique[\"CPEb-T(+)\"] = pd.to_numeric(merged_unique[\"CPEb-T(+)\"], errors=\"coerce\")\n",
        "    # Create a new boolean column \"excel_data_ok\" that is True when there is a valid sheet name\n",
        "    # and the \"CPEb-T(+)\" value is not NaN.\n",
        "    merged_unique[\"excel_data_ok\"] = merged_unique[\"sheet_name\"].notna() & (~merged_unique[\"CPEb-T(+)\"].isna())\n",
        "    # Duplicate the \"excel_data_ok\" column into a \"match\" column for easier reference.\n",
        "    merged_unique[\"match\"] = merged_unique[\"excel_data_ok\"]\n",
        "\n",
        "    # Step 6: Outlier detection on the \"CPEb-T(+)\" column.\n",
        "    # Create a mask for rows with valid Excel data.\n",
        "    valid_mask = merged_unique[\"excel_data_ok\"]\n",
        "    # Use the is_outlier_3sigma function with a z-score threshold of 2.5 to flag outliers in the \"CPEb-T(+)\" column.\n",
        "    outlier_mask = is_outlier_3sigma(merged_unique.loc[valid_mask, \"CPEb-T(+)\"], z_threshold=2.5)\n",
        "    # Initialize a new column \"is_outlier\" with False for all rows.\n",
        "    merged_unique[\"is_outlier\"] = False\n",
        "    # For rows with valid Excel data, set \"is_outlier\" to True where the outlier mask is True.\n",
        "    merged_unique.loc[valid_mask, \"is_outlier\"] = outlier_mask.values\n",
        "    # Count the total number of outliers detected.\n",
        "    n_outliers = merged_unique[\"is_outlier\"].sum()\n",
        "    # Print a message: if outliers are found, print the count; otherwise, indicate that no outliers were found.\n",
        "    if n_outliers > 0:\n",
        "        print(f\"Number of outliers found: {n_outliers}\")\n",
        "    else:\n",
        "        print(\"No outliers found in this folder.\")\n",
        "\n",
        "    # Step 7: Summarize the matching between TXT and Excel data.\n",
        "    # (A) Create a summary from the TXT side: group by experiment and cartridge,\n",
        "    # count the total number of TXT files and how many have a matching Excel row.\n",
        "    summary_txt = merged_unique.groupby([\"experiment\", \"cartridge\"])[\"match\"].agg(\n",
        "        total_txt_files=\"count\", matched_excel=\"sum\"\n",
        "    ).reset_index()\n",
        "    # Calculate unmatched TXT files as the difference between total TXT files and those matched with Excel.\n",
        "    summary_txt[\"unmatched\"] = summary_txt[\"total_txt_files\"] - summary_txt[\"matched_excel\"]\n",
        "\n",
        "    # (B) Create a summary from the Excel side: count how many TXT files were referenced in the Excel data.\n",
        "    if not df_excel.empty:\n",
        "        df_excel_filtered = df_excel.drop_duplicates(subset=[\"filepath\"])\n",
        "        excel_summary = df_excel_filtered.groupby([\"excel_experiment\", \"excel_cartridge\"]).size().reset_index(name=\"excel_txt_count\")\n",
        "    else:\n",
        "        excel_summary = pd.DataFrame(columns=[\"excel_experiment\", \"excel_cartridge\", \"excel_txt_count\"])\n",
        "    # Rename columns in the Excel summary to match those in the TXT summary.\n",
        "    excel_summary = excel_summary.rename(columns={\"excel_experiment\": \"experiment\", \"excel_cartridge\": \"cartridge\"})\n",
        "    # Merge the two summaries on experiment and cartridge.\n",
        "    summary_merged = pd.merge(summary_txt, excel_summary, on=[\"experiment\", \"cartridge\"], how=\"left\")\n",
        "    # Replace any missing values in \"excel_txt_count\" with 0 and convert to integer.\n",
        "    summary_merged[\"excel_txt_count\"] = summary_merged[\"excel_txt_count\"].fillna(0).astype(int)\n",
        "\n",
        "    # Print the final summary of TXT vs. Excel matches.\n",
        "    print(\"\\nSummary of TXT vs. Excel matches:\")\n",
        "    print(summary_merged.to_string(index=False))\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# Main Loop: Process each experiment folder under the \"Data\" directory.\n",
        "# -------------------------------------------------------------------\n",
        "pd.set_option('display.float_format', lambda x: f\"{x:.9g}\")\n",
        "base_data_dir = \"Data\"\n",
        "\n",
        "# Get a list of all subdirectories in the Data directory.\n",
        "all_experiment_folders = []\n",
        "for name in os.listdir(base_data_dir):\n",
        "    full_path = os.path.join(base_data_dir, name)\n",
        "    if os.path.isdir(full_path):\n",
        "        all_experiment_folders.append(full_path)\n",
        "\n",
        "# Process each experiment folder.\n",
        "for exp_folder in all_experiment_folders:\n",
        "    process_experiment_folder(exp_folder)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "S1AsrFVHsdyZ"
      }
    }
  ]
}